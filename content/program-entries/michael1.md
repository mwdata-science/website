---
title: "Introduction to Big Data"
speaker: michael
published: true
weight: 1
program_slot: "Day 4, 10:30-12:00 am"
program_type: "lecture"
description: "From 1956 to 2021, while capacity per unit volume has increased 200 billion times, throughput has increased only 20 thousand times, and latency a mere 150 times. This logarithmic discrepancy..."
---

There are three paramount factors that drive a (data) computing paradigm, namely: capacity, throughput and latency. Loosely put, capacity refers to how much data can we store, throughput defines how fast can we transmit data, and latency is the time lapse between issuing an instruction and starting to receive data. From 1956 to 2021, while capacity per unit volume has increased 200 billion times, throughput has increased only 20 thousand times, and latency a mere 150 times. This logarithmic discrepancy amongst the paramount factors has necessitated a shift into a computing paradigm that deploys massive parallelism and batch processing. In Big Data, we consider a portfolio of technologies that are designed to store, manage and analyze data that is too large to fit on a single machine while accommodating for the growing discrepancy amongst capacity, throughput and latency. In our talk, we will specifically introduce the fundamental concept of big data; discuss legacy Hadoop technologies before winding up with Apache Spark ecosystem. Our emphasis will be on the unifying underlying modular architecture which is strikingly common to this diversity of technologies.
